# ðŸŒ™ Dreamscape

Production-grade dream analysis system with multi-agent AI and comprehensive evaluation framework.

## Overview

Dreamscape combines dream journaling with a sophisticated AI evaluation infrastructure. Multiple agents with different analysis strategies are orchestrated via LangGraph, with LiteLLM providing unified access to various LLM providers. A custom evaluation framework measures and compares agent performance across quality, consistency, and cost metrics.

## Core Features

### Multi-Agent Analysis System

- Multiple AI agents analyze dreams using different models and prompting strategies
- Agent orchestration via LangGraph workflows
- Swappable LLM providers (OpenAI, Anthropic, Ollama) through LiteLLM
- Ensemble approach combines insights from multiple agents
- Prompt caching via LiteLLM for cost optimization

### Evaluation Framework

- **Consistency testing** - Output stability across identical inputs
- **Quality scoring** - LLM-as-judge evaluation of analysis depth
- **Hallucination detection** - Identifies unsupported claims
- **A/B testing** - Systematic prompt variation comparison
- **Regression testing** - Quality monitoring across changes
- **Golden datasets** - Curated test cases with expected outputs

### Metrics & Monitoring

- Symbol detection accuracy
- Emotional tone consistency
- Insight depth scoring
- Response latency tracking
- Cost per analysis (with caching metrics)
- User satisfaction ratings

## Architecture

```
dreamscape/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ agents/              # AI agents for dream analysis
â”‚   â”‚   â”œâ”€â”€ base_agent.py           # Abstract agent interface
â”‚   â”‚   â”œâ”€â”€ symbol_agent.py         # Symbol detection
â”‚   â”‚   â”œâ”€â”€ emotion_agent.py        # Emotional analysis
â”‚   â”‚   â”œâ”€â”€ insight_agent.py        # Psychological insights
â”‚   â”‚   â””â”€â”€ ensemble_agent.py       # Multi-agent orchestration
â”‚   â”‚
â”‚   â”œâ”€â”€ evals/               # Evaluation framework
â”‚   â”‚   â”œâ”€â”€ evaluators/             # Evaluation strategies
â”‚   â”‚   â”‚   â”œâ”€â”€ consistency_eval.py
â”‚   â”‚   â”‚   â”œâ”€â”€ quality_eval.py
â”‚   â”‚   â”‚   â””â”€â”€ hallucination_eval.py
â”‚   â”‚   â”œâ”€â”€ metrics/                # Metric collectors
â”‚   â”‚   â”œâ”€â”€ datasets/               # Golden datasets
â”‚   â”‚   â””â”€â”€ reporters/              # Results & visualization
â”‚   â”‚
â”‚   â”œâ”€â”€ api/v1/              # REST API endpoints
â”‚   â”‚   â”œâ”€â”€ dreams.py               # Dream CRUD
â”‚   â”‚   â”œâ”€â”€ analysis.py             # Trigger analysis
â”‚   â”‚   â””â”€â”€ evals.py                # Run evaluations
â”‚   â”‚
â”‚   â”œâ”€â”€ services/            # Business logic layer
â”‚   â”œâ”€â”€ models/              # Pydantic schemas
â”‚   â”œâ”€â”€ db/                  # Database layer
â”‚   â”‚   â”œâ”€â”€ models/                 # SQLAlchemy models
â”‚   â”‚   â”œâ”€â”€ session.py              # Session management
â”‚   â”‚   â””â”€â”€ migrations/             # Alembic migrations
â”‚   â”‚
â”‚   â””â”€â”€ core/                # Core utilities
â”‚       â”œâ”€â”€ config.py               # Configuration
â”‚       â”œâ”€â”€ logging.py              # Structured logging
â”‚       â””â”€â”€ deps.py                 # FastAPI dependencies
â”‚
â”œâ”€â”€ tests/                   # Test suite
â”‚   â”œâ”€â”€ test_agents/
â”‚   â”œâ”€â”€ test_evals/
â”‚   â””â”€â”€ test_api/
â”‚
â””â”€â”€ scripts/                 # Utility scripts
    â”œâ”€â”€ run_evals.py
    â””â”€â”€ seed_data.py
```

## Tech Stack

### Core Backend

- **FastAPI** - Async web framework
- **PostgreSQL** - Primary database with pgvector extension
- **SQLAlchemy 2.0** - Async ORM
- **Alembic** - Database migrations
- **Pydantic v2** - Data validation & serialization
- **Redis** - Caching & rate limiting

### AI & Agent Infrastructure

- **LangGraph** - Agent orchestration and workflow management
- **LiteLLM** - Unified API for multiple LLM providers + prompt caching
- **OpenAI API** - GPT-4 for high-quality analysis
- **Anthropic API** - Claude for alternative perspectives
- **Ollama** - Local models (Qwen 2.5)
- **pgvector** - Vector embeddings for semantic search

### Evaluation & Monitoring

- **Custom eval framework** - Built from scratch for dream analysis
- **Loguru** - Structured logging
- **Prometheus** (optional) - Metrics collection
- **arq** - Background task queue for async analysis

### Development & Testing

- **uv** - Fast Python package manager
- **pytest** - Testing framework
- **pytest-asyncio** - Async test support
- **httpx** - Async HTTP client
- **mypy** - Static type checking

## Key Technical Decisions

**Why LangGraph?**

- Production-ready agent orchestration
- State management for multi-step workflows
- Built for agentic patterns (no LangChain baggage)

**Why LiteLLM?**

- Unified API across OpenAI/Anthropic/Ollama
- Built-in prompt caching (reduces costs)
- Fallback & retry logic
- Usage tracking out of the box

**Why Custom Evals?**

- Domain-specific metrics for dream analysis
- Full control over evaluation logic
- Understanding eval patterns from first principles

## Project Goals

- Multi-agent dream analysis system with LangGraph orchestration
- Production-quality evaluation framework for measuring AI output quality
- Systematic comparison of different models and prompting strategies
- Cost optimization through caching and intelligent model selection
- Comprehensive metrics for quality, consistency, and performance
